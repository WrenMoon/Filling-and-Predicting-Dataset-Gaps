import pandas as pd
import numpy as np
from numpy import float32
from matplotlib.pyplot import xticks
from matplotlib import pyplot as plt
from sklearn.metrics import mean_absolute_percentage_error

TodasEstaciones = pd.read_csv('Chlor_A data.csv',index_col=0,parse_dates=True)
TodasEstaciones.head()

start_date = '2012-10-01'
end_date = '2013-01-08'

training_start = '2012-10-01'
training_end = '2013-01-08'

Est1 = 'Est1'
Est2 = 'Est2'
Est3 = 'Est3'

TodasEstaciones.loc[start_date:end_date].plot(subplots=True, figsize=(12, 8)); plt.legend(loc='best')
xticks(rotation='vertical')

import datetime
#we create a date column to extract the week number
TodasEstaciones['date']=TodasEstaciones.index
#apply a lambda function to the whole panda dataframe column
TodasEstaciones['week'] = TodasEstaciones['date'].apply(lambda x: x.isocalendar()[1])
#drop the date column because we dont need it
del TodasEstaciones['date']
#let see our dataframe
TodasEstaciones.head()

#creation of a correlation plot with seaborn
import seaborn as sns
corr = TodasEstaciones.corr()
sns.heatmap(corr, 
            xticklabels=corr.columns.values,
            yticklabels=corr.columns.values)

#Definition of training sets

X_train = TodasEstaciones.loc[training_start:training_end,[Est1,Est3,'week']].astype(float32).values#,'week']] # Est 1, 3 and #week
y_train = TodasEstaciones.loc[training_start:training_end,Est2].astype(float32).values # Est 2

# Import `StandardScaler` from `sklearn.preprocessing`
from sklearn.preprocessing import StandardScaler

# Define the scaler 
scaler = StandardScaler().fit(X_train)

# Scale the train set
X_train = scaler.transform(X_train)

X_train[:20]

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense


model = Sequential()

model.add(Dense(600, activation='linear', input_shape=(3,)))
model.add(Dense(200, activation='linear'))
model.add(Dense(100, activation='linear'))
model.add(Dense(10, activation='linear'))
model.add(Dense(1, activation='linear'))
model.summary()

model.compile(loss='mean_squared_error',
              optimizer='adam',
              metrics=['accuracy'])
                   
model.fit(X_train, y_train,epochs=200,verbose=2)

y_pred = model.predict(X_train)
y_pred[:10]

# Calculate MAPE
mape = mean_absolute_percentage_error(y_train, y_pred)
print(f"Mean Absolute Percentage Error (MAPE): {mape * 100:.2f}%")

# Plot predicted data
plt.plot(TodasEstaciones.loc[training_start:training_end].index, y_pred, label='Predicted')


# Plot actual data from Est2 column
TodasEstaciones[Est2].loc[start_date:end_date].plot()

# Add MAPE text to plot
plt.text(
    0.5, 0.9, f"MAPE: {mape * 100:.2f}%", 
    transform=plt.gca().transAxes, 
    fontsize=12, 
    color='red',
    bbox=dict(facecolor='white', edgecolor='red', boxstyle='round,pad=0.5')
)

# Set figure size
plt.gcf().set_size_inches(12, 8)

# Set y-axis limit
plt.ylim(0, 0.4)

# Show legend and plot
plt.legend(loc='best')
plt.show()

#Get the prediction for the train set
X_missing = TodasEstaciones.loc[start_date:end_date,[Est1,Est3,'week']].astype(float32).values

# Import `StandardScaler` from `sklearn.preprocessing`
from sklearn.preprocessing import StandardScaler

# Define the scaler 
scaler = StandardScaler().fit(X_missing)

# Scale the train set
X_missing = scaler.transform(X_missing)

y_missing = model.predict(X_missing)

TodasEstaciones['Est2_Completed']=TodasEstaciones[Est2]
TodasEstaciones['Est2_Completed'].loc[start_date:end_date]=y_missing

TodasEstaciones.loc[start_date:end_date,[Est1,Est2,'Est2_Completed',Est3]].plot(subplots=True, 
                                                   figsize=(15, 10)); plt.legend(loc='best')
xticks(rotation='vertical')
plt.ylim(0,50)